<title>Cartpole geo</title>
<script src="NeuralNetwork.js"></script>
<canvas id="box" width="500" height="500" style="border:0px solid black">
</canvas>
<style>
    body{
    margin:0;
    overflow: hidden;
    background-color: aqua;
    }
</style>
<script>
    var box = document.getElementById("box")
    box.width = innerWidth
    box.height = innerHeight
    pen = box.getContext("2d")
    pen.fillStyle = "rgb(255,0,255)"
    pen.fillRect(0,0,box.width,box.height)
    let cycles = 1
    //random color
    function GiveColor(){
        return `rgba(${Math.floor(Math.random() * (255 - 0 + 1) + 0)},${Math.floor(Math.random() * (255 - 0 + 1) + 0)},${Math.floor(Math.random() * (255 - 0 + 1) + 0)},1)`
    }
    //copy neural network
    function CopyNetwork(network) {
        let newNet = new NeuralNetwork(network.size)

        // Deep copy weights, biases
        for (let l = 0; l < network.layers.length; l++) {
            for (let i = 0; i < network.layers[l].weights.length; i++) {
                for (let j = 0; j < network.layers[l].weights[i].length; j++) {
                    newNet.layers[l].weights[i][j] = network.layers[l].weights[i][j]
                }
            }
            for (let i = 0; i < network.layers[l].biases.length; i++) {
                newNet.layers[l].biases[i] = network.layers[l].biases[i]
            }
        }
        return newNet
    }
    //find highest index of array
    function argmax(arr){
        let max = arr[0]
        let index = 0
        for(let i = 0; i < arr.length; i++){
            if(arr[i] > max){
                max = arr[i]
                index = i
            }
        }
        return index
    }
    //find highest value of array
    function MaxValue(arr){
        let max = arr[0]
        for(let i = 0; i < arr.length; i++){
            if(arr[i] > max){
                max = arr[i]
            }
        }
        return max
    }
    function DeepCopyExperience(expArray) {
        return expArray.map(exp => ({
            state: [...exp.state],
            action: exp.action,          
            reward: exp.reward,          
            nextState: [...exp.nextState], 
            done: exp.done               
        }))
    }
    
    class Cartpole{
        constructor(brain){
            //the cart
            this.x = (box.width/2)-20
            this.y = (box.height/2)-40
            this.width = 80
            this.height = 40
            this.color = GiveColor()//"cyan"
            this.speed = 0
            this.acc = 0
            this.force = 1.5
            this.left = false
            this.right = false
            //the pole
            this.pole = {
                x: this.x + this.width/2,
                y: this.y + this.height/2,
                length: 200,
                angle: Math.PI/2 + (Math.random()*0.2-0.1),
                color: this.color,
                vel:0,
                acc:0
            }
            this.DoDraw = true
            //the AI
            this.inputs = []
            this.brain = CopyNetwork(brain)
            this.Qbrain = CopyNetwork(brain)
            //Q learning stuff
            this.experience = []
            this.experienceSize = 10000
            this.sampleSize = 256
            this.oldState = []
            this.newState = []
            this.eps = 0.999
            this.epsDecay = 0.999
            this.updateCount = 0
            //time limit
            this.timer = 0
            this.timerLimit = 1000
            this.isHoldingUp = false
            this.updateTime = 50
            this.updateFrame = 0
            this.rewards = []
            this.totalReward = 0
            this.episodes = 1
            this.reward_epsisodes = []
            //nerd stuff
            this.text = `regular_run_`
            this.isFirstTime = false
            this.savedExp = []
            this.savedDataNo = 1
            this.beginSaving = false
            this.saved = false
            this.endexplore = false
            this.isRegular = true
        }
        reset(){
            this.x = (box.width/2)-20
            this.y = (box.height/2)-40
            this.speed = 0
            this.acc = 0
            this.pole.angle = Math.PI/2 + (Math.random()*0.2-0.1)
            this.pole.vel = 0
            this.pole.acc = 0
            this.timer = 0
            this.isHoldingUp = false
        }
        think(){
            //inputs
            this.inputs[0] = (this.x + this.width/2) / box.width
            this.inputs[1] = this.speed / 20
            this.inputs[2] = Math.sin(this.pole.angle)
            this.inputs[3] = Math.cos(this.pole.angle)
            this.inputs[4] = this.pole.vel / 5
            this.inputs[5] = this.pole.acc / 10 
        }
        draw(){
            pen.fillStyle = this.color
            pen.fillRect(this.x,this.y,this.width,this.height)
            //pole
            pen.strokeStyle = this.pole.color
            pen.lineWidth = 5
            pen.beginPath()
            pen.moveTo(this.pole.x,this.pole.y)
            pen.lineTo(this.pole.x + this.pole.length*Math.cos(this.pole.angle),this.pole.y + this.pole.length*Math.sin(this.pole.angle))
            pen.stroke()
            //the ball on top
            pen.beginPath()
            pen.arc(this.pole.x + this.pole.length*Math.cos(this.pole.angle),this.pole.y + this.pole.length*Math.sin(this.pole.angle),20,0,Math.PI*2)
            pen.fill()
            //i wanna give the cart a face
            //eyes
            pen.fillStyle = "white"
            pen.beginPath()
            pen.arc(this.x + this.width/4,this.y + this.height/2,10,0,Math.PI*2)
            pen.fill()
            pen.beginPath()
            pen.arc(this.x + 3*this.width/4,this.y + this.height/2,10,0,Math.PI*2)
            pen.fill()
            //derpy eyes
            pen.fillStyle = "black"
            pen.beginPath()
            pen.arc((this.x + this.width/4)-5,this.y + this.height/2,8,0,Math.PI*2)
            pen.fill()
            pen.beginPath()
            pen.arc((this.x + 3*this.width/4)+5,this.y + this.height/2,8,0,Math.PI*2)
            pen.fill()
            // UWU mouth
            pen.strokeStyle = "black"
            pen.lineWidth = 2
            pen.beginPath()
            pen.arc((this.x + this.width/2)-5,this.y + (this.height/2)+this.height/4,5,0,Math.PI)
            pen.stroke()
            pen.beginPath()
            pen.arc((this.x + this.width/2)+5,this.y + (this.height/2)+this.height/4,5,0,Math.PI)
            pen.stroke()



        }
        update(){
            //increase time limit
            this.timer++
            //thinking
            this.think()
            //AI
            let outputs
            if (Math.random() < this.eps) {
                // explore: pick a random action
                let action = Math.floor(Math.random() * 3) // 0, 1, or 2
                outputs = [0,0,0]
                outputs[action] = 1  // one-hot, just to be consistent
            } else {
                // exploit: use the brain network
                outputs = this.brain.FeedForward(this.inputs)
            }
            let action = argmax(outputs)
            switch(action){
                case 0:
                    this.left = true
                    this.right = false
                    break
                case 1:
                    this.left = false
                    this.right = false
                    break
                case 2:
                    this.left = false
                    this.right = true
                    break
            }
            //moving
            this.pole.x = this.x + this.width/2
            this.speed += this.acc
            this.speed *= 0.9
            this.x += this.speed
            //pendulum motion
            //gravity
            let g = 0.99
            //update pole angle based on cart acceleration
            this.pole.acc = (g / this.pole.length) * Math.cos(this.pole.angle) + (this.acc / this.pole.length) * Math.sin(this.pole.angle)
            this.pole.vel += this.pole.acc
            this.pole.angle += this.pole.vel
            this.pole.vel *= 0.99 // damping
            //controls
            if(this.left){
                this.acc = -this.force
            }else if(this.right){
                this.acc = this.force
            }else{
                this.acc = 0
            }
            this.oldState = [...this.inputs]
            //boundaries
            if(this.x <= 0){
                this.x = 0.01
                this.speed = 0
            }else if(this.x >= box.width - this.width){
                this.x = box.width - this.width - 0.01
                this.speed = 0
            }
            //get the new state
            this.think()
            this.newState = [...this.inputs]
            // Calculate reward
            let done =  this.timer > this.timerLimit
            const angleFromUpright = this.pole.angle + Math.PI/2

            // reward = uprightness - angular velocity penalty + survival bonus
            const reward = 
                (Math.cos(angleFromUpright))       // upright
                - 0.5 * Math.abs(this.pole.vel)   // penalize spinning
                + 0.05                            // small time bonus
            //find the best rewards
            this.rewards.push(reward)
            for(let i = 0; i < this.rewards.length; i++){
                this.totalReward += this.rewards[i]/1000
            }
            this.experience.push({
                state: this.oldState,
                action: action,
                reward: reward,
                nextState: this.newState,
                done: done
            })
             // Maintain experience buffer
             if(this.experience.length > this.experienceSize){
                //first time it's full? save it
                if(!this.isFirstTime){
                    let data = DeepCopyExperience(this.experience)
                    this.savedExp.push({
                        exp_batch_0:data
                    })
                    //we'll now start saving every 100 episodes
                    this.beginSaving = true
                    this.isFirstTime = true
                }
                this.experience.shift()
             }
             if(this.episodes % 100 === 0 && this.beginSaving && !this.saved){
                let data = DeepCopyExperience(this.experience)
                let name = `exp_batch_${this.savedDataNo}`
                this.savedExp.push({
                        [name]:data
                    })
                this.savedDataNo++
                this.saved = true
             }
             //exploration is done, 
             if(this.eps <= 0.01 && !this.endexplore){
                let data = this.experience
                this.savedExp.push({
                    end_exploit_session_final:data
                })
                this.endexplore = true
                this.updateTime = 500
             }
             if(done){
                this.deaths++
                this.reset()
                this.saved = false
                let BIG_REWARD = this.totalReward*this.rewards.length
                this.reward_epsisodes.push(
                    {
                        episode:this.episodes,
                        reward: BIG_REWARD
                    }
                )
                this.totalReward = 0
                this.rewards = []
                this.episodes++
             }
             this.updateFrame++
             if(this.updateFrame % this.updateTime === 0){
                this.UpdateNetwork()
             }
        }
        //print all that data we need
        PrintData(id) {
           //lets sAVEEE ITTT 
           let data = this.savedExp
           let episodes_rewards = this.reward_epsisodes
           let totalrewards = this.rewards
           data.push({
            total_reward_per_episode: episodes_rewards
           })
           data.push({
            reward_per_frame: totalrewards
           })
                
            // Convert object to JSON string
            let jsonData = JSON.stringify(data, null, 2)
            // Create a blob and object URL
            let blob = new Blob([jsonData], { type: "application/json" })
            let url = URL.createObjectURL(blob);

            // Create a hidden link and auto-click it
            let a = document.createElement("a")
            a.href = url
            a.download = `cartpole_${this.text}${id}.json`; // filename
            document.body.appendChild(a)
            a.click();
            document.body.removeChild(a)

            // Clean up the object URL
            URL.revokeObjectURL(url)
        }
        UpdateNetwork() {
        // QValue to store the target for training
        let QValue = 0

        // Only train if we have enough experiences
        if(this.experience.length < this.sampleSize) return

        for(let i = 0; i < this.sampleSize; i++){
            let exp = this.experience[Math.floor(Math.random()*this.experience.length)]

        if(exp.done){
            // If the episode ended, Q-value is just the reward
            QValue = exp.reward
        }else{
            // 1. Use the online network (brain) to select the action with max Q in next state
            const nextQOnline = this.brain.FeedForward(exp.nextState)
            const bestNextAction = nextQOnline.indexOf(Math.max(...nextQOnline))

            // 2. Use the target network (Qbrain) to evaluate the selected action
            const nextQTarget = this.Qbrain.FeedForward(exp.nextState)
            QValue = exp.reward + 0.99 * nextQTarget[bestNextAction]
        }

        // Prepare target values
        let TargetValues = this.brain.FeedForward(exp.state)
        TargetValues[exp.action] = QValue

        // Train the online network
        this.brain.BackProp(exp.state, TargetValues, 0.005)

        // Reset temporary variables
        TargetValues = []
        QValue = 0
    }

    // Periodically update the target network
    this.updateCount++
    if(this.updateCount % 100 === 0){
        this.Qbrain = CopyNetwork(this.brain)
    }

    // Decay epsilon
    this.eps = Math.max(0.01, this.eps * this.epsDecay)
        }
    }
    class CartpoleGeo extends Cartpole{
        constructor(brain){
            super(brain)
            this.text = `geometric_run_`
            this.stateSelectionAmt = Math.floor(Math.min(this.experience.length * 0.02,500))
            this.isRegular = false
            this.lambda = 0.01
            this.geometricWeight = 0.3
            // Add to constructor
            //this.geometricWeight = 0.8  // Start high
            this.geometricDecay = 0.9995
        }
        draw(){
            pen.fillStyle = this.color
            pen.fillRect(this.x,this.y,this.width,this.height)
            //pole
            pen.strokeStyle = this.pole.color
            pen.lineWidth = 5
            pen.beginPath()
            pen.moveTo(this.pole.x,this.pole.y)
            pen.lineTo(this.pole.x + this.pole.length*Math.cos(this.pole.angle),this.pole.y + this.pole.length*Math.sin(this.pole.angle))
            pen.stroke()
            //the ball on top
            pen.beginPath()
            pen.arc(this.pole.x + this.pole.length*Math.cos(this.pole.angle),this.pole.y + this.pole.length*Math.sin(this.pole.angle),20,0,Math.PI*2)
            pen.fill()
            //i wanna give the cart a face
            //eyes
            pen.fillStyle = "white"
            pen.beginPath()
            pen.arc(this.x + this.width/4,this.y + this.height/2,10,0,Math.PI*2)
            pen.fill()
            pen.beginPath()
            pen.arc(this.x + 3*this.width/4,this.y + this.height/2,10,0,Math.PI*2)
            pen.fill()
            //derpy eyes
            pen.fillStyle = "#333333"
            pen.beginPath()
            pen.arc((this.x + this.width/4)-5,this.y + this.height/2,8,0,Math.PI*2)
            pen.fill()
            pen.beginPath()
            pen.arc((this.x + 3*this.width/4)+5,this.y + this.height/2,8,0,Math.PI*2)
            pen.fill()
            // UWU mouth
            pen.strokeStyle = "black"
            pen.lineWidth = 2
            pen.beginPath()
            pen.arc((this.x + this.width/2)-5,this.y + (this.height/2)+this.height/4,5,0,Math.PI)
            pen.stroke()
            pen.beginPath()
            pen.arc((this.x + this.width/2)+5,this.y + (this.height/2)+this.height/4,5,0,Math.PI)
            pen.stroke()
        }
        //Mirror Action
        MirrorAction(action){
            switch (action) {
                case 0:
                    return 2
                    break;
                case 1:
                    return 1
                    break;
                case 2:
                    return 0
                    break;
            }
        }
        MirrorState(state) {
            return [
                1 - state[0],        // Mirror x position
                -state[1],           // Reverse velocity
                state[2],            // Keep sin(angle) same (symmetric)
                -state[3],           // Reverse cos(angle)
                -state[4],           // Reverse angular velocity
                -state[5]            // Reverse angular acceleration
            ]
        }
        //makes code cleaner
        ComputeQValue(exp){
            let QValue = 0
            if(exp.done){
                // If the episode ended, Q-value is just the reward
                QValue = exp.reward
            }else{
                // 1. Use the online network (brain) to select the action with max Q in next state
                const nextQOnline = this.brain.FeedForward(exp.nextState)
                const bestNextAction = nextQOnline.indexOf(Math.max(...nextQOnline))

                // 2. Use the target network (Qbrain) to evaluate the selected action
                const nextQTarget = this.Qbrain.FeedForward(exp.nextState)
                QValue = exp.reward + 0.99 * nextQTarget[bestNextAction]
            }
            //
            let TargetValues = this.brain.FeedForward(exp.state)
            TargetValues[exp.action]= QValue
            return TargetValues
        }
        UpdateNetwork() {
            if(this.experience.length < this.sampleSize) return
            
            for(let i = 0; i < this.sampleSize; i++) {
                let exp = this.experience[Math.floor(Math.random()*this.experience.length)]
                
                // Regular DQN update
                let QValue = this.ComputeQValue(exp)
                let TargetValues = this.brain.FeedForward(exp.state)
                TargetValues[exp.action] = QValue[exp.action]
                this.brain.BackProp(exp.state, TargetValues, 0.005*(1-this.geometricWeight))
                
                // Enhanced geometric update
                let MirrorState = this.MirrorState(exp.state)
                let MirrorNextState = this.MirrorState(exp.nextState)
                let MirrorAct = this.MirrorAction(exp.action)
                
                // Compute Q for mirrored experience
                let MirrorQValue = 0
                if(exp.done) {
                    MirrorQValue = exp.reward  // Same reward due to symmetry
                } else {
                    const nextQOnline = this.brain.FeedForward(MirrorNextState)
                    const bestNextAction = argmax(nextQOnline)
                    const nextQTarget = this.Qbrain.FeedForward(MirrorNextState)
                    MirrorQValue = exp.reward + 0.99 * nextQTarget[bestNextAction]
                }
                
                let TargetValuesGeo = this.brain.FeedForward(MirrorState)
                TargetValuesGeo[MirrorAct] = MirrorQValue
                
                this.brain.BackProp(MirrorState, TargetValuesGeo, 0.005 * this.geometricWeight)
        }


            // Periodically update the target network
            this.updateCount++
            if(this.updateCount % 100 === 0){
                this.Qbrain = CopyNetwork(this.brain)
            }

            // Decay epsilon
            this.eps = Math.max(0.01, this.eps * this.epsDecay)
            //
            //this.geometricWeight = Math.max(0.1, this.geometricWeight * this.geometricDecay)
            this.geometricWeight = Math.min(0.9, this.geometricWeight * 1.001)
    }
    }
    let cartpoles = []
    let pairs = 1
    for(let i = 0; i < pairs; i++){
        let b = new NeuralNetwork([6, 32, 3],0.9,0.999)
        cartpoles.push(new Cartpole(b))
        cartpoles.push(new CartpoleGeo(b))
    }
    let DrawAll = true
    let DrawId = 0
    document.addEventListener("keypress",(e)=>{
        switch (e.keyCode) {
            //D
            case 100:
                if(DrawId != cartpoles.length-1){
                    DrawId++
                }else{
                    DrawId = 0
                }
                break;
                //A
            case 97:
                if(DrawId != 0){
                    DrawId--
                }else{
                    DrawId = cartpoles.length-1
                }
                break;
            case 113:
                if(DrawAll){
                    DrawAll = false
                }else{
                    DrawAll = true
                }
                break
            case 119:
                let BestAiId = 0
                for(let i = 0; i < cartpoles.length; i++){
                    if(cartpoles[i].totalReward > cartpoles[BestAiId].totalReward){
                        BestAiId = i
                    }
                }
                DrawId = BestAiId
        }
    })
    //How to order the agents from best to worse
    let count = 1
    function findBest(){
        let copycartpoles = [...cartpoles]
        let TopListRegular = []
        let TopListGeometric = []
        //we'll do this until Top list is full
        while(TopListRegular.length != pairs){
            let HighestScore = 0
            let HighestIndex = 0
            for(let i = 0; i < copycartpoles.length; i++){
                let reward  = 0
                for(let j = 0; j < copycartpoles[i].reward_epsisodes.length; j++){
                    reward += copycartpoles[i].reward_epsisodes[j].reward
                }
                if(reward > HighestScore){
                    HighestScore = reward
                    HighestIndex = i
                }
            }
            //now we see if it's regular or not and take its pair
            if(copycartpoles[HighestIndex].isRegular){
                TopListRegular.push(copycartpoles[HighestIndex])
                TopListGeometric.push(copycartpoles[HighestIndex+1])
                //remove them from the list
                copycartpoles.splice(HighestIndex, 2)
            }else{
                TopListGeometric.push(copycartpoles[HighestIndex])
                TopListRegular.push(copycartpoles[HighestIndex-1])
                //remove them from the list
                copycartpoles.splice(HighestIndex-1, 2)

            }

        }
        //Now we print the data
        TopListRegular.forEach((cartpole,i)=>{cartpole.PrintData(count)})
        TopListGeometric.forEach((cartpole,i)=>{cartpole.PrintData(count)})

    }
    function Reset(){
        cartpoles = []
        for(let i = 0; i < pairs; i++){
            let b = new NeuralNetwork([6, 32, 3],0.9,0.999)
            cartpoles.push(new Cartpole(b))
            cartpoles.push(new CartpoleGeo(b))
        }
        HaveCollected = false
        count++
    }
    let HaveCollected = false
    function loop(){
        //updating
        for(let i = 0; i < cycles; i++){
            cartpoles.forEach((cartpole,i)=>{
                cartpole.update()
            })
            if(cartpoles[0].episodes == 1000 && !HaveCollected){
                console.log("WE DID ONE")
                //we collect data
                findBest()
                HaveCollected = true
                if(count <= 100){
                    Reset()
                }
            }
        }
        //drawing
        pen.fillStyle = "rgb(125,125,255)"
        pen.fillRect(0,0,box.width,box.height)
        cartpoles.forEach(cartpole=>{
            if(cartpole.DoDraw){
                cartpole.draw()
            }
        })
        //recalling
        requestAnimationFrame(loop)
    }
    loop()
</script>